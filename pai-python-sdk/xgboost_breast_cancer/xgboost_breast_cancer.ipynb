{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 使用 PAI Python SDK 训练和部署 XGBoost 模型\n",
    "\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/) 是基于决策树的梯度提升算法([Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting))的高效工程实现，是一个流行的机器学习库，它能够处理大的数据集合，并且做了许多训练性能优化工作。\n",
    "\n",
    "在这个教程示例中，我们将使用PAI Python SDK，在PAI上完成XGBoost模型的训练，然后将输出的模型部署为在线推理服务，并进行调用测试。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step1: 准备工作\n",
    "\n",
    "我们需要首先安装 PAI Python SDK 以运行本示例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "!python -m pip install --upgrade alipai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "SDK 需要配置访问阿里云服务需要的 AccessKey，以及当前使用的工作空间和OSS Bucket。在 PAI SDK 安装之后，通过在 **命令行终端** 中执行以下命令，按照引导配置密钥，工作空间等信息。\n",
    "\n",
    "\n",
    "```shell\n",
    "\n",
    "# 以下命令，请在 命令行终端 中执行.\n",
    "\n",
    "python -m pai.toolkit.config\n",
    "\n",
    "```\n",
    "\n",
    "我们可以通过以下代码验证当前的配置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 验证安装\n",
    "import pai\n",
    "from pai.session import get_default_session\n",
    "\n",
    "print(pai.__version__)\n",
    "\n",
    "sess = get_default_session()\n",
    "\n",
    "assert sess.workspace_name is not None\n",
    "print(sess.workspace_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: 准备数据集\n",
    "\n",
    "我们将使用[Breast Cancer数据集](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))，训练和测试XGBoost模型。准备数据集的步骤如下：\n",
    "\n",
    "1. 通过 `scikit-learn` 下载和拆分 Breast Cancer 数据集，使用 `csv` 格式保存到本地。\n",
    "\n",
    "2. 将本地数据集上传到OSS Bucket上，获得数据集的OSS URI，供云上执行的训练作业使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用SKLearn下载和拆分数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 安装 sklearn， 用于数据集下载和切分\n",
    "!{sys.executable} -m pip install --quiet  scikit-learn\n",
    "\n",
    "# 创建数据集目录\n",
    "!mkdir -p ./train_data\n",
    "!mkdir -p ./test_data\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = datasets.load_breast_cancer(as_frame=True)\n",
    "\n",
    "train, test = train_test_split(df.frame, test_size=0.3)\n",
    "\n",
    "train_data_local = \"./train_data/train.csv\"\n",
    "test_data_local = \"./test_data/train.csv\"\n",
    "\n",
    "train.to_csv(train_data_local, index=False)\n",
    "test.to_csv(test_data_local, index=False)\n",
    "\n",
    "print(f\"train data local path: {train_data_local}\")\n",
    "print(f\"test data local path: {test_data_local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上传数据集到OSS Bucket。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传数据集到OSS Bucket\n",
    "from pai.common.oss_utils import upload\n",
    "\n",
    "\n",
    "# 上传训练数据到OSS\n",
    "train_data = upload(\n",
    "    train_data_local,\n",
    "    \"pai/xgboost-example/train_data/\",\n",
    "    sess.oss_bucket,\n",
    ")\n",
    "\n",
    "\n",
    "test_data = upload(\n",
    "    test_data_local,\n",
    "    \"pai/xgboost-example/test_data/\",\n",
    "    sess.oss_bucket,\n",
    ")\n",
    "\n",
    "print(f\"train data: {train_data}\")\n",
    "print(f\"test data: {test_data}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: 提交训练作业\n",
    "\n",
    "通过PAI Python SDK提供`Estimator`，用户可以将训练脚本，提交到PAI创建一个训练作业，获得输出模型，主要流程包括：\n",
    "\n",
    "1. 用户编写训练作业脚本\n",
    "\n",
    "训练脚本负责模型代码的编写，它需要遵循PAI训练作业的规则获取作业超参，读取输入数据，并且将需要保存模型到指定的输出目录。\n",
    "\n",
    "2. 构建`Estimator`对象\n",
    "\n",
    "通过`Estimator` API，用户配置训练作业使用的脚本，镜像，超参，以及机器实例类型等信息。\n",
    "本地的脚本会有Estimator上传到OSS Bucket，然后加载到训练作业内。\n",
    "\n",
    "3. 调用`Estimator.fit`API提交作业\n",
    "\n",
    "通过`.fit`提交一个训练作业，默认`.fit`方法会等到作业停止之后，才会退出，作业结束后，用户可以通过`estimator.model_data()`获得输出模型OSS URI路径。\n",
    "\n",
    "更加完整的介绍请参考 [文档: 提交训练作业](https://pai-sdk.oss-cn-shanghai.aliyuncs.com/pai/doc/latest/user-guide/estimator.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过XGboost提供的SKlearn API，构建了一个XGBoost的训练脚本：\n",
    "\n",
    "- 训练作业默认接收两个输入Channel: train 和 test，训练脚本会从 `/ml/input/data/{channel_name}` 中读取训练数据。\n",
    "\n",
    "- 训练结束之后，训练脚本需要将模型写出到到 `/ml/output/model` 目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p xgb_src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile xgb_src/train.py\n",
    "\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "TRAINING_BASE_DIR = \"/ml/\"\n",
    "TRAINING_OUTPUT_MODEL_DIR = os.path.join(TRAINING_BASE_DIR, \"output/model/\")\n",
    "\n",
    "\n",
    "def load_dataset(channel_name):\n",
    "    path = os.path.join(TRAINING_BASE_DIR, \"input/data\", channel_name)\n",
    "    if not os.path.exists(path):\n",
    "        return None, None\n",
    "\n",
    "    # use first file in the channel dir.\n",
    "    file_name = next(\n",
    "        iter([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]),\n",
    "        None,\n",
    "    )\n",
    "    if not file_name:\n",
    "        logging.warning(f\"Not found input file in channel path: {path}\")\n",
    "        return None, None\n",
    "\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=file_path,\n",
    "        sep=\",\",\n",
    "    )\n",
    "\n",
    "    train_y = df[\"target\"]\n",
    "    train_x = df.drop([\"target\"], axis=1)\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"XGBoost train arguments\")\n",
    "    # 用户指定的任务参数\n",
    "    parser.add_argument(\n",
    "        \"--n_estimators\", type=int, default=500, help=\"The number of base model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--objective\", type=str, help=\"Objective function used by XGBoost\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--max_depth\", type=int, default=3, help=\"The maximum depth of the tree.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--eta\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"Step size shrinkage used in update to prevents overfitting.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_metric\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Evaluation metrics for validation data\"\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # 加载数据集\n",
    "    train_x, train_y = load_dataset(\"train\")\n",
    "    print(\"Train dataset: train_shape={}\".format(train_x.shape))\n",
    "    test_x, test_y = load_dataset(\"test\")\n",
    "    if test_x is None or test_y is None:\n",
    "        print(\"Test dataset not found\")\n",
    "        eval_set = [(train_x, train_y)]\n",
    "    else:\n",
    "        eval_set = [(train_x, train_y), (test_x, test_y)]\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=args.max_depth,\n",
    "        eta=args.eta,\n",
    "        n_estimators=args.n_estimators,\n",
    "        objective=args.objective,\n",
    "    )\n",
    "    clf.fit(train_x, train_y, eval_set=eval_set, eval_metric=args.eval_metric)\n",
    "\n",
    "    model_path = os.environ.get(\"PAI_OUTPUT_MODEL\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    clf.save_model(os.path.join(model_path, \"model.json\"))\n",
    "    print(f\"Save model succeed: model_path={model_path}/model.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Estimator提交训练作业\n",
    "\n",
    "通过 Estimator， 我们将以上构建的训练脚本 (xgb_src/train.py) 上传到 OSS上，通过`fit` 提交一个在云端执行XGBoost训练作业。 fit API接收的inputs分别是之前上传的训练和测试的数据，会被挂载到作业容器中（分别挂载到 `/ml/input/data/{channel_name}/`)，供训练脚本读取输入数据。\n",
    "\n",
    "提交之后，SDK 会打印作业的详情URL，并且打印作业日志，直到作业退出（成功，失败，或是停止）。用户可以点击作业URL查看任务详情，执行日志，模型的Metric，机器资源使用率等信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.estimator import Estimator\n",
    "from pai.image import retrieve\n",
    "\n",
    "\n",
    "# 获取PAI提供的XGBoost训练镜像\n",
    "image_uri = retrieve(\"xgboost\", framework_version=\"latest\").image_uri\n",
    "print(image_uri)\n",
    "\n",
    "# 构建一个Estimator实例\n",
    "est = Estimator(\n",
    "    # 作业启动脚本\n",
    "    command=\"python train.py $PAI_USER_ARGS\",\n",
    "    # 作业脚本的本地文件夹路径，会被打包上传到OSS\n",
    "    source_dir=\"./xgb_src/\",\n",
    "    image_uri=image_uri,\n",
    "    # 作业超参: 会通过Command arguments的方式传递给到作业脚本\n",
    "    hyperparameters={\n",
    "        \"n_estimator\": 100,\n",
    "        \"criterion\": \"gini\",\n",
    "        \"max_depth\": 5,\n",
    "        \"eval_metric\": \"auc\",\n",
    "    },\n",
    "    # 作业使用的机器实例\n",
    "    instance_type=\"ecs.c6.large\",\n",
    ")\n",
    "\n",
    "# 使用上传到OSS的训练数据作为作业的数据\n",
    "est.fit(\n",
    "    inputs={\n",
    "        \"train\": train_data,  # train_data 将被挂载到`/ml/input/data/train`目录\n",
    "        \"test\": test_data,  # test_data 将被挂载到`/ml/input/data/test`目录\n",
    "    },\n",
    ")\n",
    "print(est.model_data())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: 部署模型\n",
    "\n",
    "以上训练获得模型，我们将使用[预置XGBoost Processor](https://help.aliyun.com/document_detail/470490.html)部署为一个在线服务。主要流程包括:\n",
    "\n",
    "1. 通过构建一个InferenceSpec\n",
    "\n",
    "InferenceSpec负责描述模型如何部署为一个在线服务，例如模型使用镜像部署，还是使用processor部署等。\n",
    "\n",
    "2. 构建Model对象\n",
    "\n",
    "Model对象可以直接部署服务，也可以通过`.register`注册到PAI的模型仓库。\n",
    "\n",
    "3. 使用`Model.deploy`部署在线服务。\n",
    "\n",
    "通过指定服务名称，机器实例类型，部署一个新的在线推理服务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.model import Model, InferenceSpec\n",
    "from pai.predictor import Predictor\n",
    "\n",
    "from pai.common.utils import random_str\n",
    "import os\n",
    "\n",
    "\n",
    "# 使用模型文件地址以及 InferenceSpec 构建一个Model对象\n",
    "m = Model(\n",
    "    # `est.model_data()`返回的是模型文件所在的OSS目录的URI，XGBoost processor需要传递具体的模型文件。\n",
    "    model_data=os.path.join(est.model_data(), \"model.json\"),\n",
    "    inference_spec=InferenceSpec(processor=\"xgboost\"),\n",
    ")\n",
    "\n",
    "\n",
    "# 部署服务\n",
    "p: Predictor = m.deploy(\n",
    "    service_name=\"example_xgb_{}\".format(random_str(6)),\n",
    "    instance_type=\"ecs.c6.xlarge\",\n",
    "    # 启动的服务实例个数。\n",
    "    instance_count=1,\n",
    "    # 按照 每一个服务的资源使用量，而不是机器类型创建服务。\n",
    "    # instance_resource_config=ResourceConfig(\n",
    "    #     cpu=2,\n",
    "    #     memory=4000,\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: 测试在线服务\n",
    "\n",
    "`Model.deploy`方法返回一个 `Predictor` 对象，`Predictor.predict`方法支持向创建的推理服务发送推理请求，拿到预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.service_name)\n",
    "\n",
    "test_x = test.drop([\"target\"], axis=1)\n",
    "\n",
    "p.predict(test_x.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在测试结束后，删除服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.delete_service()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 1800
  },
  "kernelspec": {
   "display_name": "pai-dev-py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "63703143536f433679c5464335316251eaa13807b3fcc3854dae32f2699871d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
